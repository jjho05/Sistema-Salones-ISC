# M√©todo de Optimizaci√≥n: Machine Learning

## üìö √çndice

1. [Introducci√≥n](#introducci√≥n)
2. [Fundamentos Te√≥ricos](#fundamentos-te√≥ricos)
3. [Arquitectura del Modelo](#arquitectura-del-modelo)
4. [Ingenier√≠a de Caracter√≠sticas](#ingenier√≠a-de-caracter√≠sticas)
5. [Funci√≥n Objetivo y M√©tricas](#funci√≥n-objetivo-y-m√©tricas)
6. [Restricciones](#restricciones)
7. [Algoritmo de Optimizaci√≥n](#algoritmo-de-optimizaci√≥n)
8. [F√≥rmulas Matem√°ticas](#f√≥rmulas-matem√°ticas)
9. [Implementaci√≥n](#implementaci√≥n)
10. [Comparaci√≥n con Otros M√©todos](#comparaci√≥n-con-otros-m√©todos)

---

## Introducci√≥n

El m√©todo de **Machine Learning (ML)** para optimizaci√≥n de salones utiliza t√©cnicas de aprendizaje supervisado y no supervisado para aprender patrones √≥ptimos de asignaci√≥n a partir de datos hist√≥ricos y la optimizaci√≥n manual del profesor.

### Ventajas del Enfoque ML

- ‚úÖ **Aprende de experiencia**: Captura el conocimiento impl√≠cito del profesor
- ‚úÖ **Adaptativo**: Mejora con m√°s datos
- ‚úÖ **Escalable**: Maneja restricciones complejas mediante features
- ‚úÖ **R√°pido**: Una vez entrenado, genera soluciones en segundos
- ‚úÖ **Flexible**: F√°cil agregar nuevas restricciones

### Desventajas

- ‚ùå **Requiere datos de entrenamiento**: Necesita ejemplos de buenas asignaciones
- ‚ùå **No garantiza √≥ptimo global**: Es una aproximaci√≥n heur√≠stica
- ‚ùå **Caja negra parcial**: Menos interpretable que modelos matem√°ticos

---

## Fundamentos Te√≥ricos

### 1. Aprendizaje Supervisado

El problema se modela como **clasificaci√≥n multi-clase** donde:

- **Entrada (X)**: Caracter√≠sticas de una asignaci√≥n (grupo, materia, d√≠a, hora, profesor)
- **Salida (y)**: Sal√≥n asignado
- **Objetivo**: Aprender la funci√≥n $f: X \rightarrow y$ que minimiza el error

**Funci√≥n de Aprendizaje:**

$$
\hat{f} = \arg\min_{f \in \mathcal{F}} \sum_{i=1}^{n} L(y_i, f(x_i)) + \lambda R(f)
$$

Donde:
- $L$ = funci√≥n de p√©rdida (cross-entropy)
- $R(f)$ = regularizaci√≥n
- $\lambda$ = par√°metro de regularizaci√≥n
- $n$ = n√∫mero de ejemplos de entrenamiento

### 2. Aprendizaje por Refuerzo (Componente)

Para optimizaci√≥n iterativa, usamos **Q-Learning** modificado:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]
$$

Donde:
- $s$ = estado actual (asignaciones parciales)
- $a$ = acci√≥n (asignar sal√≥n a grupo)
- $r$ = recompensa (negativa si viola restricciones)
- $\alpha$ = tasa de aprendizaje
- $\gamma$ = factor de descuento

### 3. Clustering para Patrones

Usamos **K-Means** para identificar patrones de asignaci√≥n:

$$
\arg\min_{C} \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2
$$

Donde:
- $C_i$ = cluster $i$
- $\mu_i$ = centroide del cluster $i$
- $k$ = n√∫mero de clusters

---

## Arquitectura del Modelo

### Modelo H√≠brido: Ensemble de 3 Componentes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           ENTRADA: Asignaci√≥n a Optimizar           ‚îÇ
‚îÇ  (Grupo, Materia, D√≠a, Hora, Profesor, Contexto)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                     ‚îÇ
        ‚ñº                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Clasificador ‚îÇ    ‚îÇ   Regressor   ‚îÇ
‚îÇ  Multi-Clase  ‚îÇ    ‚îÇ  de Calidad   ‚îÇ
‚îÇ  (Random      ‚îÇ    ‚îÇ  (Gradient    ‚îÇ
‚îÇ   Forest)     ‚îÇ    ‚îÇ   Boosting)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                    ‚îÇ
        ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    ‚îÇ                               ‚îÇ
        ‚ñº    ‚ñº                               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Votaci√≥n  ‚îÇ                  ‚îÇ  Optimizador   ‚îÇ
    ‚îÇ  Ponderada ‚îÇ                  ‚îÇ  de           ‚îÇ
    ‚îÇ            ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  Restricciones ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SALIDA: Sal√≥n √ìptimo Asignado      ‚îÇ
‚îÇ  + Score de Confianza               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componente 1: Clasificador Random Forest

**Prop√≥sito**: Predecir el sal√≥n m√°s probable

**F√≥rmula de Predicci√≥n:**

$$
\hat{y} = \text{mode}\left(\{h_1(x), h_2(x), ..., h_T(x)\}\right)
$$

Donde:
- $h_t$ = √°rbol de decisi√≥n $t$
- $T$ = n√∫mero total de √°rboles
- mode = moda (valor m√°s frecuente)

**Ventajas**:
- Robusto a overfitting
- Maneja features categ√≥ricas y num√©ricas
- Proporciona importancia de features

### Componente 2: Gradient Boosting Regressor

**Prop√≥sito**: Predecir score de calidad de asignaci√≥n

**F√≥rmula Iterativa:**

$$
F_m(x) = F_{m-1}(x) + \nu \cdot h_m(x)
$$

Donde:
- $F_m$ = modelo en iteraci√≥n $m$
- $\nu$ = learning rate
- $h_m$ = √°rbol que ajusta residuos

**Score de Calidad:**

$$
\text{Score}(x, y) = -\left(w_1 \cdot \text{Movimientos} + w_2 \cdot \text{Inv√°lidos} + w_3 \cdot \text{Conflictos}\right)
$$

### Componente 3: Optimizador de Restricciones

**Prop√≥sito**: Ajustar predicciones para satisfacer restricciones hard

**Algoritmo de Ajuste:**

```python
def ajustar_restricciones(asignacion_predicha, restricciones):
    if viola_restriccion_hard(asignacion_predicha):
        candidatos = generar_candidatos_validos(asignacion_predicha)
        return max(candidatos, key=lambda c: score_modelo(c))
    return asignacion_predicha
```

---

## Ingenier√≠a de Caracter√≠sticas

### Features de Entrada (X)

#### 1. Features del Grupo

| Feature | Tipo | Descripci√≥n | F√≥rmula |
|---------|------|-------------|---------|
| `grupo_codigo` | Categ√≥rica | C√≥digo del grupo | - |
| `es_primer_semestre` | Binaria | ¬øEs 1er semestre? | $\mathbb{1}[\text{c√≥digo}[0] = '1']$ |
| `num_estudiantes` | Num√©rica | Tama√±o del grupo | - |
| `semestre` | Ordinal | Semestre (1-9) | $\text{int}(\text{c√≥digo}[0])$ |

#### 2. Features de la Materia

| Feature | Tipo | Descripci√≥n | F√≥rmula |
|---------|------|-------------|---------|
| `materia_nombre` | Categ√≥rica | Nombre de materia | - |
| `horas_teoria` | Num√©rica | Horas te√≥ricas | - |
| `horas_practica` | Num√©rica | Horas pr√°cticas | - |
| `requiere_lab` | Binaria | ¬øNecesita lab? | $\mathbb{1}[\text{horas\_practica} > 0]$ |
| `tipo_clase` | Categ√≥rica | Teor√≠a/Lab/Mixta | - |

#### 3. Features Temporales

| Feature | Tipo | Descripci√≥n | F√≥rmula |
|---------|------|-------------|---------|
| `dia_semana` | Categ√≥rica | Lun-Vie | - |
| `bloque_horario` | Categ√≥rica | HHMM | - |
| `hora_inicio` | Num√©rica | Hora (7-19) | $\text{int}(\text{bloque}[:2])$ |
| `es_hora_pico` | Binaria | ¬øHora pico? | $\mathbb{1}[10 \leq \text{hora} \leq 14]$ |

#### 4. Features del Profesor

| Feature | Tipo | Descripci√≥n | F√≥rmula |
|---------|------|-------------|---------|
| `profesor_id` | Categ√≥rica | ID del profesor | - |
| `tiene_movilidad_reducida` | Binaria | ¬øMovilidad limitada? | - |
| `num_materias` | Num√©rica | Materias que imparte | - |
| `preferencia_piso` | Categ√≥rica | Baja/Alta/Sin pref | - |

#### 5. Features Contextuales

| Feature | Tipo | Descripci√≥n | F√≥rmula |
|---------|------|-------------|---------|
| `salon_anterior` | Categ√≥rica | Sal√≥n en hora anterior | - |
| `salon_siguiente` | Categ√≥rica | Sal√≥n en hora siguiente | - |
| `num_cambios_dia` | Num√©rica | Cambios de sal√≥n en d√≠a | - |
| `distancia_anterior` | Num√©rica | Distancia desde anterior | $d(\text{salon}_t, \text{salon}_{t-1})$ |

#### 6. Features del Sal√≥n Candidato

| Feature | Tipo | Descripci√≥n | F√≥rmula |
|---------|------|-------------|---------|
| `salon_codigo` | Categ√≥rica | C√≥digo del sal√≥n | - |
| `tipo_salon` | Categ√≥rica | Teor√≠a/Lab/Inv√°lido | - |
| `piso` | Categ√≥rica | Baja/Alta/P1/P2 | - |
| `capacidad` | Num√©rica | Capacidad del sal√≥n | - |
| `disponible` | Binaria | ¬øEst√° libre? | - |
| `uso_actual` | Num√©rica | % de uso en horario | $\frac{\text{horas\_ocupadas}}{\text{horas\_totales}}$ |

### Feature Engineering Avanzado

#### Interacciones de Features

$$
\text{feature\_interaccion} = f_1 \times f_2
$$

Ejemplos:
- `es_primer_semestre √ó salon_tipo`
- `hora_inicio √ó dia_semana`
- `profesor_movilidad √ó salon_piso`

#### Features Agregadas

$$
\text{movimientos\_profesor} = \sum_{t=1}^{T} \mathbb{1}[\text{salon}_t \neq \text{salon}_{t-1}]
$$

$$
\text{cambios\_piso} = \sum_{t=1}^{T} \mathbb{1}[\text{piso}_t \neq \text{piso}_{t-1}]
$$

---

## Funci√≥n Objetivo y M√©tricas

### Funci√≥n de P√©rdida Compuesta

$$
\mathcal{L}_{\text{total}} = \alpha \mathcal{L}_{\text{clasificaci√≥n}} + \beta \mathcal{L}_{\text{restricciones}} + \gamma \mathcal{L}_{\text{optimizaci√≥n}}
$$

#### 1. P√©rdida de Clasificaci√≥n

$$
\mathcal{L}_{\text{clasificaci√≥n}} = -\frac{1}{N}\sum_{i=1}^{N} \log P(y_i | x_i)
$$

#### 2. P√©rdida de Restricciones (Penalizaci√≥n)

$$
\mathcal{L}_{\text{restricciones}} = \sum_{r \in R_{\text{hard}}} w_r \cdot \mathbb{1}[\text{viola}(r)]
$$

Donde:
- $R_{\text{hard}}$ = conjunto de restricciones hard
- $w_r$ = peso de penalizaci√≥n (muy alto, ej: 1000)

#### 3. P√©rdida de Optimizaci√≥n (Soft Constraints)

$$
\mathcal{L}_{\text{optimizaci√≥n}} = \sum_{c \in C_{\text{soft}}} w_c \cdot \text{costo}(c)
$$

**Costos espec√≠ficos:**

$$
\text{costo\_movimientos} = \sum_{p \in P} \sum_{t=1}^{T_p-1} d(\text{salon}_{p,t}, \text{salon}_{p,t+1})
$$

$$
\text{costo\_inv√°lidos} = 1000 \times |\{\text{asignaciones a AV/E11}\}|
$$

$$
\text{costo\_capacidad} = \sum_{i} \max(0, \text{estudiantes}_i - \text{capacidad}_i)
$$

### M√©tricas de Evaluaci√≥n

| M√©trica | F√≥rmula | Objetivo |
|---------|---------|----------|
| **Accuracy** | $\frac{\text{correctas}}{total}$ | Maximizar |
| **F1-Score** | $2 \cdot \frac{P \cdot R}{P + R}$ | Maximizar |
| **MAE Movimientos** | $\frac{1}{N}\sum\|\text{pred} - \text{real}\|$ | Minimizar |
| **Violaciones Hard** | $\sum \mathbb{1}[\text{viola}]$ | = 0 |
| **Score Optimizaci√≥n** | $-\mathcal{L}_{\text{optimizaci√≥n}}$ | Maximizar |

---

## Restricciones

### Restricciones Hard (Obligatorias)

#### H1: Grupos de Primer Semestre

$$
\forall g \in G_1, \forall t_1, t_2 \in T_{\text{teor√≠a}}: \text{salon}_{g,t_1} = \text{salon}_{g,t_2}
$$

**Implementaci√≥n ML**: Feature `es_primer_semestre` con penalizaci√≥n alta

#### H2: Salones Inv√°lidos

$$
\forall i: \text{salon}_i \notin \{\text{AV1, AV2, AV4, AV5, E11}\}
$$

**Implementaci√≥n ML**: Filtrado post-predicci√≥n + penalizaci√≥n $10^6$

#### H3: Capacidad

$$
\forall i: \text{estudiantes}_i \leq \text{capacidad}(\text{salon}_i)
$$

**Implementaci√≥n ML**: Feature `capacidad_suficiente` + validaci√≥n

#### H4: Tipo de Sal√≥n

$$
\text{Si horas\_practica} > 0 \Rightarrow \text{salon} \in \text{Labs}
$$

**Implementaci√≥n ML**: Feature `tipo_compatible` + regla de negocio

#### H5: Disponibilidad

$$
\forall i, j: i \neq j \Rightarrow (\text{salon}_i, \text{hora}_i) \neq (\text{salon}_j, \text{hora}_j)
$$

**Implementaci√≥n ML**: Matriz de disponibilidad + validaci√≥n

#### H6: Movilidad Reducida (Futuro)

$$
\text{Si profesor\_movilidad} = \text{reducida} \Rightarrow \text{salon} \in \text{Planta Baja} \cup \text{Labs P1}
$$

**Implementaci√≥n ML**: Feature `compatible_movilidad` + filtrado

### Restricciones Soft (Deseables)

#### S1: Minimizar Movimientos de Profesores

$$
\min \sum_{p \in P} \sum_{t=1}^{T_p-1} \mathbb{1}[\text{salon}_{p,t} \neq \text{salon}_{p,t+1}]
$$

**Peso**: $w_1 = 10$

#### S2: Minimizar Cambios de Piso

$$
\min \sum_{p \in P} \sum_{t=1}^{T_p-1} \mathbb{1}[\text{piso}_{p,t} \neq \text{piso}_{p,t+1}]
$$

**Peso**: $w_2 = 5$

#### S3: Minimizar Distancia Total

$$
\min \sum_{p \in P} \sum_{t=1}^{T_p-1} d(\text{salon}_{p,t}, \text{salon}_{p,t+1})
$$

**Peso**: $w_3 = 3$

#### S4: Balancear Uso de Salones

$$
\min \text{Var}(\{\text{uso}(\text{salon}_s) : s \in S\})
$$

**Peso**: $w_4 = 2$

---

## Algoritmo de Optimizaci√≥n

### Fase 1: Entrenamiento

```python
ALGORITMO: Entrenar_Modelo_ML

ENTRADA:
    - D_inicial: Horario inicial
    - D_optimizado: Horario optimizado por profesor
    - restricciones: Lista de restricciones

SALIDA:
    - modelo_entrenado: Modelo ML listo para predecir

PASOS:
1. Extraer features de D_inicial y D_optimizado
   X_train, y_train = extraer_features(D_inicial, D_optimizado)

2. Agregar features contextuales
   X_train = agregar_contexto(X_train, D_inicial)

3. Entrenar clasificador Random Forest
   clf = RandomForest(n_estimators=100, max_depth=20)
   clf.fit(X_train, y_train)

4. Entrenar regressor de calidad
   reg = GradientBoosting(n_estimators=100, learning_rate=0.1)
   scores = calcular_scores_calidad(D_optimizado)
   reg.fit(X_train, scores)

5. Validar con cross-validation
   cv_score = cross_val_score(clf, X_train, y_train, cv=5)

6. Retornar modelo ensemble
   return Ensemble(clf, reg, restricciones)
```

### Fase 2: Predicci√≥n y Optimizaci√≥n

```python
ALGORITMO: Optimizar_Horario_ML

ENTRADA:
    - D_inicial: Horario a optimizar
    - modelo: Modelo entrenado
    - restricciones: Restricciones a satisfacer

SALIDA:
    - D_optimizado: Horario optimizado

PASOS:
1. Inicializar horario vac√≠o
   D_opt = {}

2. Ordenar asignaciones por prioridad
   asignaciones = ordenar_por_prioridad(D_inicial)
   # Prioridad: 1er semestre > labs > teor√≠a

3. Para cada asignaci√≥n a en asignaciones:
   
   3.1. Extraer features
       x = extraer_features(a, D_opt)
   
   3.2. Predecir top-k salones candidatos
       candidatos = modelo.predict_proba(x).top_k(k=5)
   
   3.3. Filtrar por restricciones hard
       candidatos_validos = [c for c in candidatos 
                            if satisface_hard(c, restricciones)]
   
   3.4. Si no hay candidatos v√°lidos:
       candidatos_validos = buscar_alternativas(a, D_opt)
   
   3.5. Evaluar calidad de cada candidato
       scores = [modelo.score(x, c) for c in candidatos_validos]
   
   3.6. Seleccionar mejor candidato
       mejor = candidatos_validos[argmax(scores)]
   
   3.7. Asignar y actualizar
       D_opt[a] = mejor
       actualizar_contexto(D_opt, a, mejor)

4. Optimizaci√≥n local (hill climbing)
   D_opt = mejorar_local(D_opt, modelo, restricciones)

5. Retornar soluci√≥n
   return D_opt
```

### Fase 3: Refinamiento Iterativo

```python
ALGORITMO: Refinar_Soluci√≥n

ENTRADA:
    - D_opt: Soluci√≥n inicial
    - modelo: Modelo ML
    - max_iter: Iteraciones m√°ximas

SALIDA:
    - D_final: Soluci√≥n refinada

PASOS:
1. score_actual = evaluar(D_opt)

2. Para i = 1 hasta max_iter:
   
   2.1. Seleccionar asignaci√≥n aleatoria a
   
   2.2. Generar vecinos (cambiar sal√≥n de a)
       vecinos = generar_vecinos(D_opt, a)
   
   2.3. Evaluar vecinos
       scores_vecinos = [evaluar(v) for v in vecinos]
   
   2.4. Si max(scores_vecinos) > score_actual:
       D_opt = vecinos[argmax(scores_vecinos)]
       score_actual = max(scores_vecinos)
   
   2.5. Si no mejora en k iteraciones:
       break

3. return D_opt
```

---

## F√≥rmulas Matem√°ticas Detalladas

### 1. C√°lculo de Distancia entre Salones

**Matriz de Distancias** (ejemplo simplificado):

$$
D = \begin{bmatrix}
0 & 1 & 2 & 3 & 10 \\
1 & 0 & 1 & 2 & 9 \\
2 & 1 & 0 & 1 & 8 \\
3 & 2 & 1 & 0 & 7 \\
10 & 9 & 8 & 7 & 0
\end{bmatrix}
$$

**Funci√≥n de Distancia:**

$$
d(s_1, s_2) = \begin{cases}
0 & \text{si } s_1 = s_2 \\
1 & \text{si mismo piso, adyacentes} \\
2 & \text{si mismo piso, no adyacentes} \\
5 & \text{si diferente piso, mismo edificio} \\
10 & \text{si diferente edificio}
\end{cases}
$$

### 2. Score de Calidad de Asignaci√≥n

$$
Q(a, s) = \sum_{i=1}^{n} w_i \cdot f_i(a, s)
$$

Donde:
- $f_1$ = compatibilidad de tipo (0 o 1)
- $f_2$ = utilizaci√≥n del sal√≥n (0-1)
- $f_3$ = distancia normalizada (0-1)
- $f_4$ = preferencia hist√≥rica (0-1)

**Normalizaci√≥n:**

$$
f_{\text{norm}} = \frac{f - f_{\min}}{f_{\max} - f_{\min}}
$$

### 3. Probabilidad de Asignaci√≥n (Softmax)

$$
P(\text{salon}_j | x) = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}
$$

Donde:
- $z_j = w_j^T x + b_j$ (score del sal√≥n $j$)
- $K$ = n√∫mero total de salones

### 4. Importancia de Features (Random Forest)

$$
\text{Importancia}(f) = \frac{1}{T} \sum_{t=1}^{T} \sum_{n \in N_t} \Delta i(n, f)
$$

Donde:
- $\Delta i(n, f)$ = reducci√≥n de impureza en nodo $n$ por feature $f$
- $N_t$ = nodos del √°rbol $t$

---

## Implementaci√≥n

### Tecnolog√≠as y Librer√≠as

```python
# Core ML
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix

# Optimizaci√≥n
import numpy as np
import pandas as pd
from scipy.optimize import minimize

# Visualizaci√≥n
import matplotlib.pyplot as plt
import seaborn as sns
```

### Estructura de Clases

```python
class OptimizadorML:
    def __init__(self, restricciones):
        self.clf = RandomForestClassifier(...)
        self.reg = GradientBoostingRegressor(...)
        self.restricciones = restricciones
        self.encoder = LabelEncoder()
        self.scaler = StandardScaler()
    
    def entrenar(self, X, y):
        """Entrena el modelo con datos hist√≥ricos"""
        pass
    
    def predecir(self, X):
        """Predice sal√≥n √≥ptimo para asignaci√≥n"""
        pass
    
    def optimizar(self, horario_inicial):
        """Optimiza horario completo"""
        pass
    
    def evaluar(self, horario):
        """Eval√∫a calidad de horario"""
        pass
```

---

## Comparaci√≥n con Otros M√©todos

| Aspecto | Machine Learning | ILP (Matem√°tico) | Gen√©tico |
|---------|-----------------|------------------|----------|
| **Garant√≠a de √ìptimo** | ‚ùå No | ‚úÖ S√≠ (si converge) | ‚ùå No |
| **Velocidad** | ‚úÖ R√°pido (post-entrenamiento) | ‚ùå Lento | ‚ö†Ô∏è Medio |
| **Escalabilidad** | ‚úÖ Excelente | ‚ùå Limitada | ‚úÖ Buena |
| **Interpretabilidad** | ‚ö†Ô∏è Media | ‚úÖ Alta | ‚ùå Baja |
| **Requiere Datos** | ‚úÖ S√≠ | ‚ùå No | ‚ùå No |
| **Manejo Restricciones** | ‚ö†Ô∏è Aproximado | ‚úÖ Exacto | ‚ö†Ô∏è Aproximado |
| **Adaptabilidad** | ‚úÖ Alta | ‚ùå Baja | ‚úÖ Alta |
| **Complejidad Impl.** | ‚ö†Ô∏è Media | ‚úÖ Baja | ‚ö†Ô∏è Media |

### Cu√°ndo Usar Cada M√©todo

**Machine Learning**:
- ‚úÖ Hay datos hist√≥ricos de buenas asignaciones
- ‚úÖ Se necesita velocidad en producci√≥n
- ‚úÖ Las restricciones son complejas pero flexibles
- ‚úÖ Se espera que el problema evolucione

**ILP (Matem√°tico)**:
- ‚úÖ Se necesita garant√≠a de optimalidad
- ‚úÖ El problema es de tama√±o peque√±o-mediano
- ‚úÖ Las restricciones son r√≠gidas y bien definidas
- ‚úÖ No hay datos hist√≥ricos

**Gen√©tico**:
- ‚úÖ El espacio de b√∫squeda es muy grande
- ‚úÖ Se acepta una buena soluci√≥n (no necesariamente √≥ptima)
- ‚úÖ Las restricciones son dif√≠ciles de modelar matem√°ticamente
- ‚úÖ Se necesita diversidad de soluciones

---

## Resultados Esperados

### M√©tricas de √âxito

1. **Asignaciones Inv√°lidas**: 0 (vs 51 inicial)
2. **Movimientos de Profesores**: Reducci√≥n del 30-50%
3. **Accuracy de Predicci√≥n**: > 85%
4. **Tiempo de Ejecuci√≥n**: < 30 segundos
5. **Satisfacci√≥n de Restricciones Hard**: 100%

### Diferencias con Otros M√©todos

Los resultados del ML ser√°n **diferentes** porque:

- **Aprende patrones** del profesor (puede replicar decisiones sub√≥ptimas)
- **Balancea** m√∫ltiples objetivos de forma diferente
- **Prioriza** features seg√∫n importancia aprendida
- **Puede ser m√°s conservador** (prefiere asignaciones conocidas)

---

## Pr√≥ximos Pasos

1. ‚úÖ Documentaci√≥n completa
2. ‚è≥ Implementar extracci√≥n de features
3. ‚è≥ Entrenar modelo con datos existentes
4. ‚è≥ Implementar algoritmo de optimizaci√≥n
5. ‚è≥ Validar con restricciones
6. ‚è≥ Generar comparativa vs inicial y profesor
7. ‚è≥ Crear visualizaciones y PDF

---

## Referencias

- Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5-32.
- Friedman, J. H. (2001). Greedy Function Approximation: A Gradient Boosting Machine.
- Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction.
- Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning.
